# ğŸ§ª Model Testing Suite

This folder contains comprehensive UI-based testing tools for all AI models used in the application.

## ğŸ“ Folder Structure

```
model-testing/
â”œâ”€â”€ README.md (this file)
â”œâ”€â”€ test-dashboard/          # Main test dashboard UI
â”œâ”€â”€ text-models/             # Text processing models
â”‚   â”œâ”€â”€ mistral-7b/
â”‚   â”œâ”€â”€ zephyr-7b/
â”‚   â””â”€â”€ llama-70b-groq/
â”œâ”€â”€ image-generation/        # Image generation models
â”‚   â”œâ”€â”€ stable-diffusion-xl/
â”‚   â””â”€â”€ stable-diffusion-v1-5/
â”œâ”€â”€ image-understanding/      # Image-to-text models
â”‚   â”œâ”€â”€ blip-captioning/
â”‚   â””â”€â”€ blip-vqa/
â”œâ”€â”€ audio-processing/        # Audio models
â”‚   â”œâ”€â”€ whisper-stt/
â”‚   â””â”€â”€ bark-tts/
â””â”€â”€ code-generation/         # Code generation models
    â”œâ”€â”€ codellama-7b/
    â””â”€â”€ deepseek-coder/
```

## ğŸ”‘ Environment Variables

Make sure these are set in your `.env` file (lines 14-17):

```env
HUGGINGFACE_API_KEY=your_huggingface_key_here
REPLICATE_API_TOKEN=your_replicate_token_here
GROQ_API_KEY=your_groq_key_here
```

**Note:** These should also be set in the backend environment for production use.

## ğŸš€ Quick Start

1. **Access the Test Dashboard:**
   - Navigate to `/model-testing` in your application
   - Or use the direct route: `/model-testing/dashboard`

2. **Run Individual Tests:**
   - Each test case has its own folder with a README
   - Each README contains input examples and expected outputs
   - Use the UI components to test each model interactively

3. **View Test Results:**
   - All tests show real-time results
   - Compare actual output vs expected output
   - Debug issues using the detailed logs

## ğŸ“ Test Case Format

Each test case folder contains:

- **README.md** - Test documentation with:
  - Model information
  - Input examples
  - Expected outputs
  - Common issues and solutions
- **TestComponent.tsx** - UI component for testing
- **test-config.json** - Test configuration and test cases

## ğŸ¯ Testing Workflow

1. **Select a model category** from the dashboard
2. **Choose a specific model** to test
3. **Review the README** for input/output expectations
4. **Run test cases** using the UI
5. **Compare results** with expected outputs
6. **Debug issues** using the provided troubleshooting guides

## ğŸ” Debugging

If a test fails:

1. Check the model's README for common issues
2. Verify API keys are correctly configured
3. Review the error logs in the UI
4. Check Supabase function logs for backend errors
5. Verify API quotas/limits haven't been exceeded

## ğŸ“Š Test Coverage

- âœ… Text Processing (Q&A, Summarization, Translation)
- âœ… Image Generation (Text-to-Image)
- âœ… Image Understanding (Captioning, VQA)
- âœ… Audio Processing (Speech-to-Text, Text-to-Speech)
- âœ… Code Generation (Multi-language)

## ğŸ”— Related Documentation

- [API Keys Setup Guide](../Debugging/01-Setup-Configuration/API_KEYS_SETUP_GUIDE.md)
- [Multimodal Models Debug Guide](../Debugging/05-Testing-Debugging/MULTIMODAL_MODELS_DEBUG_GUIDE.md)
- [Supabase 406 Error Fix](../Debugging/01-Setup-Configuration/SUPABASE_406_ERROR_FIX.md)

